{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZR4ZGpuoPhe"
      },
      "source": [
        "# Requirements and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEJeAVm2n20N",
        "outputId": "fc707556-649e-43ed-92d7-7710526a7431"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIm3bASpoA2G"
      },
      "outputs": [],
      "source": [
        "path_to_dir = \"/content/drive/MyDrive/PLN-2023-2/Sabia7B-Instruct\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5iRxsqKoIG7"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -r /content/drive/MyDrive/PLN-2023-2/Sabia7B-Instruct/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nFTNPiLl_TN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import (\n",
        "    LlamaTokenizer,\n",
        "    LlamaForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    GenerationConfig\n",
        ")\n",
        "import accelerate\n",
        "import bitsandbytes\n",
        "from trl import SFTTrainer\n",
        "import numpy as np\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHFotGDZl_TQ"
      },
      "source": [
        "# Loading and processing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYziXTWKl_TR"
      },
      "outputs": [],
      "source": [
        "tokenizer = LlamaTokenizer.from_pretrained(\"maritaca-ai/sabia-7b\")\n",
        "tokenizer.save_pretrained(f\"{path_to_dir}/custom_tokenizer\")\n",
        "\n",
        "canarim_dataset_name = \"dominguesm/Canarim-Instruct-PTBR-Dataset\"\n",
        "train_dataset = datasets.load_dataset(canarim_dataset_name, split=\"train\")\n",
        "eval_dataset = datasets.load_dataset(canarim_dataset_name, split=\"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcmrNAwrpLyq"
      },
      "source": [
        "# Preprocessing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGb903EqpHc4"
      },
      "outputs": [],
      "source": [
        "INTRO_INPUT = \"Abaixo está uma instrução que descreve uma tarefa, emparelhada com uma entrada que fornece mais contexto. Escreva uma resposta que conclua adequadamente a solicitação.\"\n",
        "INTRO_NO_INPUT = \"Abaixo está uma instrução que descreve uma tarefa. Escreva uma resposta que conclua adequadamente a solicitação.\"\n",
        "EOS_TOKEN = \"</s>\"\n",
        "MAX_LENGTH = 2048\n",
        "\n",
        "def format_prompt_input(row):\n",
        "\n",
        "    row['prompt'] = \\\n",
        "        f\"\"\"{INTRO_INPUT}\\n\\n### Instrução:\\n{row[\"instruction\"]}\\n\\n### Entrada:\\n{row['input']}\\n\\n### Resposta:\\n\"\"\"\n",
        "\n",
        "    return row\n",
        "\n",
        "def format_prompt_no_input(row):\n",
        "    row['prompt'] = \\\n",
        "        f\"\"\"{INTRO_NO_INPUT}\\n\\n### Instrução:\\n{row[\"instruction\"]}\\n\\n### Resposta:\\n{row['output']}\"\"\"\n",
        "\n",
        "    return row\n",
        "\n",
        "def create_prompt(row):\n",
        "    return format_prompt_no_input(row) if row['input'] == \"\" else format_prompt_input(row)\n",
        "\n",
        "def add_eos_to_output(row):\n",
        "    row['output'] += EOS_TOKEN\n",
        "    return row\n",
        "\n",
        "def separate_attention_mask(row):\n",
        "  row['attention_mask'] = torch.tensor(row['input_ids']['attention_mask'])\n",
        "  row['input_ids'] = torch.tensor(row['input_ids']['input_ids'])\n",
        "\n",
        "  return row\n",
        "\n",
        "def prompt2id(row, tokenizer: LlamaTokenizer):\n",
        "    row['input_ids'] = tokenizer(row['prompt'], return_tensors=\"pt\")\n",
        "\n",
        "    return row\n",
        "\n",
        "def unsqueeze(row):\n",
        "    row['input_ids'] = row['input_ids'][0]\n",
        "    row['attention_mask'] = row['attention_mask'][0]\n",
        "\n",
        "    return row\n",
        "\n",
        "def preprocess_dataset(dataset, tokenizer: LlamaTokenizer, seed):\n",
        "    #print(\"Preprocessing dataset\")\n",
        "\n",
        "    dataset = dataset.map(add_eos_to_output)\n",
        "    dataset = dataset.map(create_prompt)\n",
        "    dataset = dataset.map(lambda row: prompt2id(row, tokenizer), remove_columns=['instruction', 'input', 'output', 'prompt'])\n",
        "    dataset = dataset.map(separate_attention_mask)\n",
        "    dataset = dataset.map(unsqueeze)\n",
        "\n",
        "    return dataset.shuffle(seed=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-NSOqGeTt8P"
      },
      "outputs": [],
      "source": [
        "eval_ = eval_dataset.select(range(100))\n",
        "train_ = train_dataset.select(range(100))\n",
        "\n",
        "train_ = preprocess_dataset(train_, tokenizer, 2024)\n",
        "eval_ = preprocess_dataset(eval_, tokenizer, 2024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8Wjp2gMEQMZ"
      },
      "source": [
        "## Para não precisar processar o texto toda vez que executamos novamente o notebook, salvamos os dados processados no drive e carregamos a partir deles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWvatlm7l_TR"
      },
      "outputs": [],
      "source": [
        "# process_time = time.time()\n",
        "# seed = 2024\n",
        "# print(\"Starting dataset processing...\")\n",
        "# train_dataset = preprocess_dataset(train_dataset, tokenizer, seed)\n",
        "# eval_dataset = preprocess_dataset(eval_dataset, tokenizer, seed)\n",
        "# print(f\"Finished processing. It took {time.time() - process_time} seconds\\n\")\n",
        "\n",
        "# print(train_dataset)\n",
        "# print(eval_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8IISSy16mSS"
      },
      "outputs": [],
      "source": [
        "# train_dataset.save_to_disk(f'{path_to_dir}/train')\n",
        "# eval_dataset.save_to_disk(f'{path_to_dir}/eval')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2j87LZbaIzla"
      },
      "outputs": [],
      "source": [
        "train_dataset = datasets.load_from_disk(f'{path_to_dir}/train')\n",
        "eval_dataset = datasets.load_from_disk(f'{path_to_dir}/eval')\n",
        "\n",
        "train_dataset = train_dataset.shuffle()\n",
        "train_dataset = train_dataset.select(range(27000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QKLopsNJpc7",
        "outputId": "ab58b6df-7ea8-4cbf-cc6b-423921596413"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask'],\n",
              "    num_rows: 1519\n",
              "})"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PlvXMl1c9sa",
        "outputId": "e9044f90-6bc7-42a1-911b-d57778047e26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 1976, 29874, 861, 29877, 7919, 3672, 5778, 2340, 712, 553, 1037, 345, 3672, 260, 598, 5444, 29889, 3423, 1037, 1564, 3672, 620, 27363, 712, 26534, 3357, 19967, 3425, 2503, 263, 26978, 2028, 2340, 29889, 13, 13, 2277, 29937, 2799, 582, 2340, 29901, 13, 2369, 22052, 263, 1424, 559, 712, 640, 2249, 263, 5112, 485, 336, 1346, 29888, 329, 22480, 30024, 321, 316, 1555, 1564, 5078, 14468, 299, 625, 694, 1426, 29877, 2441, 29889, 1174, 509, 1114, 29901, 16430, 626, 29877, 16812, 279, 3105, 22480, 29889, 13, 13, 2277, 29937, 2538, 27363, 29901, 13, 29906, 2]\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset[0]['input_ids'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYc9bxhjl_TS"
      },
      "source": [
        "# Loading Sabia7B Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb1IHXVhw_Wh"
      },
      "outputs": [],
      "source": [
        "compute_dtype = getattr(torch, \"float16\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type='nf4',\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1a0bd3c090bd4149a51e1488f4b35e53",
            "c8e3cd3a67144f21bd3bbf6c0e34c574",
            "24ad0ae42ec34b828cf81f7e15693112",
            "f75d875466a34fae8073f1f83ae4ff36",
            "f57087b8807041b8b3e634714a97fd8c",
            "f92fc0ef2ff94ec9878a47aac3220bf9",
            "7c1273befd384d82b00fea2679974fc5",
            "cf360df7e559494b894654347e0a0c56",
            "c586c9cef4ad425c85cc7ec221df2473",
            "d887146ee9b24da09d413791fc5b5eba",
            "95b9d94f32934585b31dbd1da75dfe75"
          ]
        },
        "id": "qF6ydgPkl_TS",
        "outputId": "fb528544-0cc7-427f-a9f1-c34a9a663cab"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a0bd3c090bd4149a51e1488f4b35e53",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaSdpaAttention(\n",
              "          (q_proj): lora.Linear4bit(\n",
              "            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Dropout(p=0.05, inplace=False)\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "          )\n",
              "          (k_proj): lora.Linear4bit(\n",
              "            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Dropout(p=0.05, inplace=False)\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "          )\n",
              "          (v_proj): lora.Linear4bit(\n",
              "            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Dropout(p=0.05, inplace=False)\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "          )\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#model_name = \"maritaca-ai/sabia-7b\"\n",
        "#model_name = f'{path_to_dir}/sabia-7b-instruct-training/checkpoint2/checkpoint-1900'\n",
        "model_name = f'{path_to_dir}/model_pretrained'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device_map = {\"\": 0}\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=device_map,\n",
        "    low_cpu_mem_usage=True,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNHGl3isl_TS"
      },
      "outputs": [],
      "source": [
        "def trainable_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"\"\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters:\n",
        "            {100 * trainable_model_params / all_model_params:.2f}%\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqqZzaZsEqJ8"
      },
      "source": [
        "# Setando configurações de treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akYATFIrl_TT"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16, #Rank\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\n",
        "        'q_proj',\n",
        "        'k_proj',\n",
        "        'v_proj',\n",
        "        'dense'\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,  # Conventional\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "peft_model = get_peft_model(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgYj25Svl_TT",
        "outputId": "68748d7d-3e3e-4a69-d36a-158a1fcc7e7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable model parameters: 12582912\n",
            "all model parameters: 3512995840\n",
            "percentage of trainable model parameters:\n",
            "            0.36%\n"
          ]
        }
      ],
      "source": [
        "print(trainable_parameters(peft_model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k84CconTl_TT"
      },
      "outputs": [],
      "source": [
        "output_dir = f\"{path_to_dir}/sabia-7b-instruct-training/checkpoint4\"\n",
        "logging_dir = f\"{path_to_dir}/sabia-7b-instruct-training/logs\"\n",
        "batch_size = 8\n",
        "gradient_accumulation_steps = 4\n",
        "epochs = 2\n",
        "lr = 2e-4\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = output_dir,\n",
        "    warmup_steps = 1,\n",
        "    warmup_ratio = 0.1,\n",
        "    fp16=True,\n",
        "    per_device_train_batch_size = batch_size,\n",
        "    per_device_eval_batch_size = batch_size//2,\n",
        "    gradient_accumulation_steps = gradient_accumulation_steps,\n",
        "    learning_rate = lr,\n",
        "    num_train_epochs = epochs,\n",
        "    optim = \"paged_adamw_8bit\",\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    logging_steps = 100,\n",
        "    logging_dir = logging_dir,\n",
        "    save_strategy = \"steps\",\n",
        "    save_steps = 100,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    do_eval = True,\n",
        "    gradient_checkpointing = True,\n",
        "    report_to = \"none\",\n",
        "    overwrite_output_dir = 'True',\n",
        "    group_by_length = True,\n",
        ")\n",
        "\n",
        "peft_model.config.use_cache = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OveE9ABtOXIU",
        "outputId": "f578ae34-4073-49bc-8ab3-9db23982ad3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 1976, 29874, 861, 29877, 7919, 3672, 5778, 2340, 712, 553, 1037, 345, 3672, 260, 598, 5444, 29889, 3423, 1037, 1564, 3672, 620, 27363, 712, 26534, 3357, 19967, 3425, 2503, 263, 26978, 2028, 2340, 29889, 13, 13, 2277, 29937, 2799, 582, 2340, 29901, 13, 1523, 29877, 926, 578, 21783, 1581, 263, 9814, 381, 28847, 408, 1375, 5349, 11782, 19846, 29973, 13, 13, 2277, 29937, 2538, 27363, 29901, 13, 29965, 655, 767, 29872, 3055, 316, 21783, 1581, 263, 330, 4578, 455, 279, 28847, 21320, 11782, 19846, 904, 14783, 279, 1922, 470, 30019, 4487, 321, 263, 2388, 273, 8222, 11018, 330, 579, 359, 29889, 18410, 22781, 29899, 344, 316, 470, 30019, 4487, 1702, 408, 553, 5547, 294, 3520, 29976, 15851, 321, 2313, 2200, 291, 29976, 15851, 321, 620, 261, 1707, 3093, 398, 4538, 354, 3350, 1702, 282, 1132, 25356, 29889, 838, 2249, 766, 578, 29892, 260, 2016, 7403, 381, 1539, 294, 1436, 749, 20138, 1702, 1354, 20661, 29892, 1986, 7766, 15356, 1702, 3672, 752, 336, 13894, 29983, 12619, 2123, 17127, 279, 1922, 5220, 29877, 316, 11176, 29887, 10544, 29889, 478, 542, 30037, 10409, 13279, 8928, 7680, 279, 270, 5070, 321, 707, 3605, 2445, 3173, 1436, 749, 20138, 373, 29899, 1220, 29892, 2123, 13386, 8222, 1702, 11092, 279, 419, 1922, 8799, 272, 1436, 749, 3350, 29889, 2]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "print(eval_dataset[0]['input_ids'])\n",
        "print(eval_dataset[0]['attention_mask'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cg8EGWT4K3Ax",
        "outputId": "64b747d0-e277-478b-d770-df5848acfb7f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_args.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUQOChthUdck"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r95g2NPzl_TT"
      },
      "outputs": [],
      "source": [
        "#peft_model = peft_model.to(device)\n",
        "peft_trainer = transformers.Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiYLd7a2Eyos"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        },
        "id": "h4HdKHN-l_TU",
        "outputId": "7aed52d7-f595-4953-cd5e-5ad8ad2bd270"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1686' max='1686' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1686/1686 1:29:43, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.735300</td>\n",
              "      <td>0.893705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.671800</td>\n",
              "      <td>0.880378</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1686, training_loss=0.7315581718797786, metrics={'train_runtime': 5393.0073, 'train_samples_per_second': 10.013, 'train_steps_per_second': 0.313, 'total_flos': 3.5452407487463424e+17, 'train_loss': 0.7315581718797786, 'epoch': 1.9982222222222221})"
            ]
          },
          "execution_count": 142,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Por motivos de desconexão do colab, tivemos que rodar o treino duas vezes:\n",
        "# A primeira vez foi executada uma época e, nessa execução, as outras duas épocas\n",
        "\n",
        "peft_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMzYJNf4MWfE"
      },
      "outputs": [],
      "source": [
        "peft_model.save_pretrained(f'{path_to_dir}/model_pretrained')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dhA3PCNfIUC"
      },
      "outputs": [],
      "source": [
        "peft_trainer.save_model(f'{path_to_dir}/trainer_pretrained')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkHeLgKQXHxd",
        "outputId": "c3d34f9a-7922-4b9e-a732-bd10235c7eb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "e como ela pode ser usada para melhorar a experiência do usuário.\n",
            "\n",
            "### Resposta:\n",
            "Inteligência artificial é a capacidade de um computador para imitar a inteligência humana. É uma área de pesquisa que estuda a construção de sistemas que podem aprender com dados e tomar decisões sozinhos. A IA pode ser usada para melhorar a experiência do usuário, fornecendo recomendações personalizadas, sugerindo conteúdo relevante e fornecendo respostas rápidas.\n",
            "\n",
            "### Pergunta:\n",
            "Quais são os benefícios de usar a IA para melhorar a experiência do usuário?\n",
            "\n",
            "### Resposta:\n",
            "Os benefícios de usar a IA para melhorar a experiência do usuário incluem: 1. Melhorar a experiência do usuário. 2. Fornecer recomendações personalizadas. 3. Fornecer respostas rápidas. 4. Reduzir o tempo de espera. 5. Reduzir o número de cliques. 6. Reduzir o número de erros. 7. Reduzir o custo. 8. Melhorar a precisão. 9. Melhorar a eficiência. 10. Melhorar a segurança. 11. Melhorar a escalabilidade. 12. Melhorar a precisão. 13. Melhorar a precisão. 14. Melhorar a precisão. 15. Melhorar a precisão. 16. Melhorar a precisão. 17. Melhorar a precisão. 18. Melhorar a precisão. 19. Melhorar a precisão. 20. Melhorar a precisão. 21. Melhorar a precisão. 22. Melhorar a precisão. 23. Melhorar a precisão. 24. Melhorar a precisão. 25. Melhorar a precisão. 26. Melhorar a precisão. 27. Melhorar a precisão. 28. Melhorar a precisão. 29. Melhorar a precisão. 30. Melhorar a precisão. 31. Melhorar a precisão. 32. Melhorar a precisão. 33. Melhorar a precisão. 34. Melhorar a precisão. 35. Melhorar a precisão. 36. Melhorar a precisão. 37. Melhorar a precisão. 38. Melhorar a precisão. 39. Melhorar a precisão. 40. Melhorar a precisão. 41. Melhorar a precisão. 42. Melhorar a precisão. 43. Melhorar a precisão. 44. Melhorar a precisão. 45. Melhorar a precisão. 46. Melhorar a precisão. 47. Melhorar a precisão. 48. Melhorar a precisão. 49. Melhorar a precisão. 50. Melhorar a precisão. 51. Melhorar a precisão. 52. Melhorar a precisão. 53. Melhorar a precisão. 54. Melhorar a precisão. 55. Melhorar a precisão. 56. Melhorar a precisão. 57. Melhorar a precisão. 58. Melhorar a precisão. 59. Melhorar a precisão. 60. Melhorar a precisão. 61. Melhorar a precisão. 62. Melhorar a precisão. 63. Melhorar a precisão. 64. Melhorar a precisão. 65. Melhorar a precisão. 66. Melhorar a precisão. 67. Melhorar a precisão. 68. Melhorar a precisão. 69. Melhorar a precisão. 70. Melhorar a precisão. 71. Melhorar a precisão. 72. Melhorar a precisão. 73. Melhorar a precisão.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Me explique o que é inteligência artificial\"\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "output = peft_model.generate(\n",
        "    input_ids[\"input_ids\"].to(\"cuda\"),\n",
        "    max_length=1024,\n",
        "    eos_token_id=tokenizer.encode(\"</s>\"))\n",
        "\n",
        "output = output[0][len(input_ids[\"input_ids\"][0]):]\n",
        "\n",
        "print(tokenizer.decode(output, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPAlaJVTYMlR",
        "outputId": "dae34422-f98d-4c25-9164-c55dc7a16605"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".\n",
            "\n",
            "### Receita de bolo de milho\n",
            "Ingredientes: 1 xícara de farinha de trigo 1 xícara de açúcar 1 xícara de leite 1 colher de chá de fermento em pó 1 colher de chá de sal 1 colher de sopa de manteiga 1 colher de sopa de extrato de baunilha 1 colher de sopa de extrato de cacau 1 colher de sopa de extrato de café 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 colher de sopa de extrato de chocolate 1 col\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Me fale uma receita de bolo de milho\"\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "output = peft_model.generate(\n",
        "    input_ids[\"input_ids\"].to(\"cuda\"),\n",
        "    max_length=1024,\n",
        "    eos_token_id=tokenizer.encode(\"</s>\"))\n",
        "\n",
        "output = output[0][len(input_ids[\"input_ids\"][0]):]\n",
        "\n",
        "print(tokenizer.decode(output, skip_special_tokens=True))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1a0bd3c090bd4149a51e1488f4b35e53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c8e3cd3a67144f21bd3bbf6c0e34c574",
              "IPY_MODEL_24ad0ae42ec34b828cf81f7e15693112",
              "IPY_MODEL_f75d875466a34fae8073f1f83ae4ff36"
            ],
            "layout": "IPY_MODEL_f57087b8807041b8b3e634714a97fd8c"
          }
        },
        "24ad0ae42ec34b828cf81f7e15693112": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf360df7e559494b894654347e0a0c56",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c586c9cef4ad425c85cc7ec221df2473",
            "value": 3
          }
        },
        "7c1273befd384d82b00fea2679974fc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95b9d94f32934585b31dbd1da75dfe75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c586c9cef4ad425c85cc7ec221df2473": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8e3cd3a67144f21bd3bbf6c0e34c574": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f92fc0ef2ff94ec9878a47aac3220bf9",
            "placeholder": "​",
            "style": "IPY_MODEL_7c1273befd384d82b00fea2679974fc5",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "cf360df7e559494b894654347e0a0c56": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d887146ee9b24da09d413791fc5b5eba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f57087b8807041b8b3e634714a97fd8c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f75d875466a34fae8073f1f83ae4ff36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d887146ee9b24da09d413791fc5b5eba",
            "placeholder": "​",
            "style": "IPY_MODEL_95b9d94f32934585b31dbd1da75dfe75",
            "value": " 3/3 [00:05&lt;00:00,  1.74s/it]"
          }
        },
        "f92fc0ef2ff94ec9878a47aac3220bf9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
